{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5007b71",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "START_INDEX = 10  # Skip first few samples\n",
    "SIGNAL_LENGTH = 1024 * 16\n",
    "SAMPLE_RATE = 20e6\n",
    "MASK_SIZE = 1024 * 16  # Mask size for segmentation\n",
    "\n",
    "# Functions for Signal Processing\n",
    "def load_real_data(sample_path):\n",
    "    \"\"\"\n",
    "    Load raw signal data from a .dat file.\n",
    "    \"\"\"\n",
    "    with open(sample_path, \"rb\") as f:\n",
    "        signal = np.fromfile(f, dtype=np.complex64)\n",
    "    return signal\n",
    "\n",
    "def load_data(signal_id):\n",
    "    \"\"\"\n",
    "    Load signal data and its corresponding metadata.\n",
    "    \"\"\"\n",
    "    signal = load_real_data(signal_id)\n",
    "    metadata_file = signal_id.with_suffix(\".json\")\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Metadata file {metadata_file} not found for signal {signal_id}\")\n",
    "    return signal[START_INDEX:], metadata, metadata_file\n",
    "\n",
    "def apply_psd(signal, Fs, NFFT):\n",
    "    \"\"\"\n",
    "    Calculate the PSD and corresponding frequencies using Welch's method.\n",
    "    \"\"\"\n",
    "    freqs, psd = welch(signal, fs=Fs, nfft=NFFT, return_onesided=False)\n",
    "    psd = np.fft.fftshift(psd)\n",
    "    freqs = np.fft.fftshift(freqs)\n",
    "    return psd, freqs\n",
    "\n",
    "def calculate_fft(signal):\n",
    "    \"\"\"\n",
    "    Calculate the FFT of the signal and return real and imaginary parts as separate channels.\n",
    "    \"\"\"\n",
    "    signal = signal[:SIGNAL_LENGTH]\n",
    "    signal = np.fft.fft(signal)\n",
    "    signal = np.fft.fftshift(signal)\n",
    "    signal /= np.max(np.abs(signal))\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b802c",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class WidebandSignalDataset(Dataset):\n",
    "    def __init__(self, signal_ids, mask_size=1024 * 16):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with signal IDs and the specified mask size.\n",
    "        \"\"\"\n",
    "        self.mask_size = mask_size\n",
    "        self.signal_ids = signal_ids\n",
    "        self.loaded_data = [self.process_signal(signal_id) for signal_id in tqdm(self.signal_ids)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.loaded_data[index]\n",
    "\n",
    "    def process_signal(self, signal_id):\n",
    "        signal, metadata, _ = load_data(signal_id)\n",
    "\n",
    "        # Ensure signal length matches SIGNAL_LENGTH\n",
    "        if len(signal) < SIGNAL_LENGTH:\n",
    "            # Pad with zeros if the signal is shorter\n",
    "            signal = np.pad(signal, (0, SIGNAL_LENGTH - len(signal)), mode='constant')\n",
    "        elif len(signal) > SIGNAL_LENGTH:\n",
    "            # Truncate if the signal is longer\n",
    "            signal = signal[:SIGNAL_LENGTH]\n",
    "\n",
    "        # Apply FFT\n",
    "        signal = np.fft.fft(signal)\n",
    "        signal = np.fft.fftshift(signal)\n",
    "        signal /= np.max(np.abs(signal))  # Normalize\n",
    "        complex_signal = torch.from_numpy(signal).type(torch.complex64).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        # Create mask with fixed size\n",
    "        masks = torch.zeros(self.mask_size, dtype=torch.float32)\n",
    "        scale_ratio = self.mask_size / SAMPLE_RATE\n",
    "        scaled_metadata = process_metadata(metadata)\n",
    "        for meta in scaled_metadata:\n",
    "            f1, f2 = meta[\"position\"]\n",
    "            x1 = int(math.floor(f1 * scale_ratio))\n",
    "            x2 = int(math.ceil(f2 * scale_ratio))\n",
    "            masks[x1:x2] = 1\n",
    "\n",
    "        return complex_signal, masks\n",
    "\n",
    "\n",
    "\n",
    "def process_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Scale metadata to the dataset's frequency and bandwidth ranges.\n",
    "    \"\"\"\n",
    "    scaled_metadata = [\n",
    "        {\n",
    "            \"position\": (\n",
    "                math.floor((SAMPLE_RATE / 2 + i[\"fc\"] - i[\"bw\"] / 2) * SIGNAL_LENGTH / SAMPLE_RATE),\n",
    "                math.ceil((SAMPLE_RATE / 2 + i[\"fc\"] + i[\"bw\"] / 2) * SIGNAL_LENGTH / SAMPLE_RATE)\n",
    "            ),\n",
    "            \"snr\": 1,  # Placeholder value\n",
    "            \"bw\": i[\"bw\"],\n",
    "            \"num\": len(metadata),\n",
    "            \"esn0\": 1,  # Placeholder value\n",
    "        }\n",
    "        for i in metadata\n",
    "    ]\n",
    "    return scaled_metadata\n",
    "\n",
    "# Dataset Splitting and Initialization\n",
    "NEW_DATA_DIR = Path(\"/data/bigred/ofh/0\")\n",
    "def get_real_signals(freq_directory):\n",
    "    return list(freq_directory.rglob(\"*.dat\"))\n",
    "\n",
    "signal_dirs = get_real_signals(NEW_DATA_DIR)\n",
    "total_signals = len(signal_dirs)\n",
    "\n",
    "train_split = int(0.80 * total_signals)\n",
    "validation_split = int(0.90 * total_signals)\n",
    "\n",
    "train, validation, test = (\n",
    "    signal_dirs[:train_split],\n",
    "    signal_dirs[train_split:validation_split],\n",
    "    signal_dirs[validation_split:]\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(validation)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5305642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = WidebandSignalDataset(signal_ids=train)\n",
    "validation_dataset = WidebandSignalDataset(signal_ids=validation)\n",
    "test_dataset = WidebandSignalDataset(signal_ids=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893c583",
   "metadata": {},
   "source": [
    "### CV-ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2001c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import complexPyTorch.complexLayers as cplx\n",
    "from typing import Optional, Callable, Type, Union, List\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> cplx.ComplexConv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return cplx.ComplexConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> cplx.ComplexConv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return cplx.ComplexConv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = cplx.ComplexBatchNorm2d(planes * self.expansion)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ComplexResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = SIGNAL_LENGTH,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(ComplexResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = cplx.ComplexBatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = cplx.ComplexConv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = cplx.ComplexLinear(512 * block.expansion, num_classes)\n",
    "        self.sigmoid = cplx.ComplexSigmoid()\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "def ComplexResNet18():\n",
    "    return ComplexResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Create the model instance\n",
    "model = ComplexResNet18()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e09e4",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f79a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0001, save_path='./path/to/model/save'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.best_model = None\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_model = model.state_dict()\n",
    "        save_path = os.path.join(self.save_path, 'best_model.pth')\n",
    "        torch.save(self.best_model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fda74",
   "metadata": {},
   "source": [
    "### Focal loss and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(ComplexFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        real_inputs = inputs.real\n",
    "        imag_inputs = inputs.imag\n",
    "        \n",
    "        real_BCE_loss = F.binary_cross_entropy(real_inputs, targets, reduction='none')\n",
    "        imag_BCE_loss = F.binary_cross_entropy(imag_inputs, targets, reduction='none')\n",
    "        \n",
    "        real_pt = torch.exp(-real_BCE_loss)\n",
    "        imag_pt = torch.exp(-imag_BCE_loss)\n",
    "        \n",
    "        real_F_loss = self.alpha * (1 - real_pt) ** self.gamma * real_BCE_loss\n",
    "        imag_F_loss = self.alpha * (1 - imag_pt) ** self.gamma * imag_BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return (torch.mean(real_F_loss) + torch.mean(imag_F_loss)) / 2\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(real_F_loss) + torch.sum(imag_F_loss)\n",
    "        else:\n",
    "            return real_F_loss + imag_F_loss\n",
    "\n",
    "# Update the IoU calculation to handle complex values\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    real_pred = (pred.real > threshold).float()\n",
    "    imag_pred = (pred.imag > threshold).float()\n",
    "    \n",
    "    combined_pred = torch.logical_or(real_pred, imag_pred).float()\n",
    "    \n",
    "    intersection = (combined_pred * target).sum(dim=1)\n",
    "    union = (combined_pred + target).sum(dim=1) - intersection\n",
    "    iou = (intersection / union).mean().item()\n",
    "    return iou\n",
    "def reshape_to_2d(data):\n",
    "    return data.view(-1, 1, 128, 128)  # Reshape to [batch, channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97635b0",
   "metadata": {},
   "source": [
    "### BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV BCE Loss Function Definition\n",
    "class ComplexValuedBCELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(ComplexValuedBCELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        real_inputs = inputs.real\n",
    "        imag_inputs = inputs.imag\n",
    "\n",
    "        # Calculate binary cross-entropy for both real and imaginary parts\n",
    "        real_BCE_loss = F.binary_cross_entropy(real_inputs, targets, reduction=self.reduction)\n",
    "        imag_BCE_loss = F.binary_cross_entropy(imag_inputs, targets, reduction=self.reduction)\n",
    "        \n",
    "        # Combine the losses (you can adjust the weighting if necessary)\n",
    "        combined_BCE_loss = (real_BCE_loss + imag_BCE_loss) / 2\n",
    "        return combined_BCE_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4063c",
   "metadata": {},
   "source": [
    "### Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66825110",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "device=\"cuda\"\n",
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    iou_scores = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(valid_loader, desc=\"Validating\"):\n",
    "            inputs = reshape_to_2d(inputs).to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate IoU\n",
    "            iou = calculate_iou(outputs, masks, threshold=0.5)\n",
    "            iou_scores.append(iou)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (outputs.real > 0.5).float()\n",
    "            correct = (preds == masks).float().sum()\n",
    "            total_correct += correct.item()\n",
    "            total_samples += masks.numel()\n",
    "\n",
    "    val_loss = running_loss / len(valid_loader)\n",
    "    mean_iou = sum(iou_scores) / len(iou_scores)\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    print(f'Validation Loss: {val_loss:.6f}')\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return val_loss, accuracy\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, initial_lr=0.001, lr_steps=[0.0001], num_epochs=50, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    epoch_durations = []\n",
    "    \n",
    "    current_lr = initial_lr\n",
    "    for lr in lr_steps:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, delta=0.001)\n",
    "        print(\"Current learning rate: \", lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "                inputs = reshape_to_2d(inputs).to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            print(f\"Training Loss: {epoch_loss:.6f}\")\n",
    "            val_loss, val_accuracy = validate_model(model, valid_loader, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            epoch_durations.append(epoch_duration)\n",
    "        if early_stopping.best_model is not None:\n",
    "            print(f\"Loading best model from lr {lr}\")\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "        \n",
    "    print(\"Training completed.\")\n",
    "    print(\"Epoch durations:\", epoch_durations)\n",
    "    return model, train_losses, val_losses, val_accuracies, epoch_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d28b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and train the ResNet-18 model\n",
    "model = ComplexResNet18().to(device)\n",
    "criterion = ComplexFocalLoss()\n",
    "\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations =train_model(model, train_loader, valid_loader, criterion, initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=3)\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838c1bc",
   "metadata": {},
   "source": [
    "### Transfer Learning Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac763e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the pre-trained model weights\n",
    "pretrained_model_path = \"path/to/model/save.pth\" #Change this model to trained model\n",
    "device=\"cuda\"\n",
    "# Initialize the model architecture\n",
    "model = ComplexResNet18().to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "checkpoint = torch.load(pretrained_model_path)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Set all layers as trainable (if needed)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f877827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a new criterion and optimizer for fine-tuning\n",
    "# You may select between Focal Loss or BCE as your criterion\n",
    "#criterion = ComplexValuedBCELoss()  # or ComplexValuedBCELoss()\n",
    "criterion = ComplexFocalLoss()\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Train the model (fine-tuning)\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations= train_model(\n",
    "    model, train_loader, valid_loader, criterion,\n",
    "    initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=3\n",
    ")\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3784964",
   "metadata": {},
   "source": [
    "### Plot Result and save the figures and json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a52e13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define save directory\n",
    "save_dir = 'CMuSeNet_results/segmentation'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the training loss figure as PNG and SVG\n",
    "plt.savefig(os.path.join(save_dir, 'training_loss.png'))\n",
    "plt.savefig(os.path.join(save_dir, 'training_loss.svg'))\n",
    "\n",
    "# Show the training loss plot\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Save the validation accuracy figure as PNG and SVG\n",
    "plt.savefig(os.path.join(save_dir, 'validation_accuracy.png'))\n",
    "plt.savefig(os.path.join(save_dir, 'validation_accuracy.svg'))\n",
    "\n",
    "# Show the validation accuracy plot\n",
    "plt.show()\n",
    "\n",
    "# Save the actual data to a JSON file\n",
    "results = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_accuracies\": val_accuracies\n",
    "}\n",
    "\n",
    "# Save JSON file\n",
    "with open(os.path.join(save_dir, 'training_validation_results.json'), 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222069ae",
   "metadata": {},
   "source": [
    "### BIG-RED Evaluation (Over entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b178984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# Create a DataLoader for the entire dataset\n",
    "BATCH_SIZE = 64  # Adjust based on available memory\n",
    "entire_dataset = WidebandSignalDataset(signal_ids=signal_dirs)  # Use all signals\n",
    "entire_loader = DataLoader(entire_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6be59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the pre-trained model weights\n",
    "pretrained_model_path = \"path/to/model/pretrained\" \n",
    "device = \"cuda\" \n",
    "\n",
    "# Initialize the model architecture\n",
    "model = ComplexResNet18().to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "checkpoint = torch.load(pretrained_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Function to evaluate accuracy\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(data_loader, desc=\"Evaluating on Entire Dataset\"):\n",
    "            inputs = reshape_to_2d(inputs).to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs.real > 0.5).float()\n",
    "\n",
    "            correct = (preds == masks).float().sum()\n",
    "            total_correct += correct.item()\n",
    "            total_samples += masks.numel()\n",
    "\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    print(f\"Overall Accuracy on Entire Dataset: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run the evaluation\n",
    "overall_accuracy = evaluate_accuracy(model, entire_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a21b4",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54736ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for evaluation\n",
    "device = \"cuda\"\n",
    "model_path = \"path/to/model/save.pth\"\n",
    "model = resnet18_1D().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ConcatDataset([\n",
    "    WidebandSignalDataset(signal_ids=train, return_snrs=True),\n",
    "    WidebandSignalDataset(signal_ids=validation, return_snrs=True),\n",
    "    WidebandSignalDataset(signal_ids=test, return_snrs=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f711d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_true(array, distance=1):\n",
    "    # Create kernel of appropriate size\n",
    "    kernel = torch.ones((1, 1, distance * 2 + 1), device=array.device)\n",
    "    array = array.unsqueeze(1).float()  # Add channel dimension\n",
    "    result = F.conv1d(array, kernel, padding=distance)\n",
    "    result = result.squeeze(1)  # Remove the extra dimension\n",
    "    return result > 0\n",
    "\n",
    "def get_true_groups(tensor, device):\n",
    "    assert tensor.dim() == 2, 'This function handles 2D tensor only'\n",
    "    all_groups = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        item = tensor[i]\n",
    "        item = torch.cat([torch.tensor([False]).to(device), item, torch.tensor([False]).to(device)])\n",
    "        diffs = item.float().diff()\n",
    "        starts = (diffs == 1).nonzero(as_tuple=True)[0]\n",
    "        ends = (diffs == -1).nonzero(as_tuple=True)[0] - 1\n",
    "        groups = [(start.item(), end.item()) for start, end in zip(starts, ends)]\n",
    "        all_groups.append(groups)\n",
    "    return all_groups\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    intersection = max(0, min(box1[1], box2[1]) - max(box1[0], box2[0]))\n",
    "    union = max(box1[1], box2[1]) - min(box1[0], box2[0])\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def match_targets(targets, preds):\n",
    "    ious = []\n",
    "    for target in targets:\n",
    "        iou_targets = []\n",
    "        for pred in preds:\n",
    "            iou_targets.append(calculate_iou(target, pred))\n",
    "        ious.append(iou_targets)\n",
    "    cost_matrix = np.array(ious)\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "    return row_ind, col_ind\n",
    "\n",
    "def calculate_matched_ious(target_boxes, prediction_boxes, matching):\n",
    "    ious = [0 for _ in target_boxes]\n",
    "    matching_dict = dict(zip(*matching))\n",
    "    for target_index, target_box in enumerate(target_boxes):\n",
    "        if target_index in matching_dict:\n",
    "            pred_index = matching_dict[target_index]\n",
    "            if pred_index < len(prediction_boxes):\n",
    "                box1 = target_box\n",
    "                box2 = prediction_boxes[pred_index]\n",
    "                ious[target_index] = calculate_iou(box1, box2)\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictor, data_loader, device=\"cuda\"):\n",
    "    iou_thresholds = [0.5, 0.7, 0.9]\n",
    "    snr_metrics = defaultdict(lambda: {\n",
    "        \"iou_sum\": 0.0,\n",
    "        \"iou_count\": 0,\n",
    "        \"recall_counts\": defaultdict(int),\n",
    "        \"total_samples\": defaultdict(int),\n",
    "        \"correct_pixels\": 0,\n",
    "        \"total_pixels\": 0\n",
    "    })\n",
    "    total_iou_sum, total_iou_count = 0.0, 0\n",
    "    total_correct_pixels, total_total_pixels = 0, 0\n",
    "    total_recall_counts = defaultdict(int)\n",
    "    total_samples = defaultdict(int)\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        if len(batch) == 3:\n",
    "            inputs, masks, snrs_in_batch = batch\n",
    "        else:\n",
    "            inputs, masks = batch\n",
    "            snrs_in_batch = [0] * len(inputs)  # Default SNR if not provided\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = predictor(inputs)\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "            mask = masks[i]\n",
    "            output = outputs[i]\n",
    "\n",
    "            # Resize output to match mask shape if necessary\n",
    "            if output.numel() != mask.numel():\n",
    "                output = output.expand_as(mask) if output.numel() == 1 else output.reshape_as(mask)\n",
    "\n",
    "            thresholded_output = (output >= 0.5).float()\n",
    "\n",
    "            correct_pixels = (thresholded_output == mask).sum().item()\n",
    "            total_pixels = mask.numel()\n",
    "            total_correct_pixels += correct_pixels\n",
    "            total_total_pixels += total_pixels\n",
    "\n",
    "            # Get SNR value and round it to the nearest integer\n",
    "            snr = snrs_in_batch[i]\n",
    "            if isinstance(snr, torch.Tensor):\n",
    "                snr = snr.item()\n",
    "            snr = int(round(snr))  # Round SNR to the nearest integer\n",
    "\n",
    "            snr_metrics[snr][\"correct_pixels\"] += correct_pixels\n",
    "            snr_metrics[snr][\"total_pixels\"] += total_pixels\n",
    "\n",
    "            target_boxes = get_true_groups(mask.unsqueeze(0), device=device)[0]\n",
    "            pred_boxes = get_true_groups(thresholded_output.unsqueeze(0), device=device)[0]\n",
    "            if not target_boxes or not pred_boxes:\n",
    "                continue\n",
    "            matching = match_targets(target_boxes, pred_boxes)\n",
    "            matched_ious = calculate_matched_ious(target_boxes, pred_boxes, matching)\n",
    "\n",
    "            snr_metrics[snr][\"iou_sum\"] += sum(matched_ious)\n",
    "            snr_metrics[snr][\"iou_count\"] += len(matched_ious)\n",
    "            total_iou_sum += sum(matched_ious)\n",
    "            total_iou_count += len(matched_ious)\n",
    "\n",
    "            for th in iou_thresholds:\n",
    "                true_positives = sum(1 for iou in matched_ious if iou >= th)\n",
    "                snr_metrics[snr][\"recall_counts\"][th] += true_positives\n",
    "                snr_metrics[snr][\"total_samples\"][th] += len(target_boxes)\n",
    "                total_recall_counts[th] += true_positives\n",
    "                total_samples[th] += len(target_boxes)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = (total_correct_pixels / total_total_pixels) * 100 if total_total_pixels > 0 else 0\n",
    "    overall_iou = total_iou_sum / total_iou_count if total_iou_count > 0 else 0\n",
    "    overall_recall = {\n",
    "        th: total_recall_counts[th] / total_samples[th] if total_samples[th] > 0 else 0\n",
    "        for th in iou_thresholds\n",
    "    }\n",
    "\n",
    "    # Print overall results\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(f\"Overall IoU Score: {overall_iou:.4f}\")\n",
    "    for th in iou_thresholds:\n",
    "        print(f\"Recall at threshold {th}: {overall_recall[th]:.4f}\")\n",
    "\n",
    "    # Print per-SNR results\n",
    "    for snr in sorted(snr_metrics.keys()):\n",
    "        metrics = snr_metrics[snr]\n",
    "        snr_accuracy = (metrics[\"correct_pixels\"] / metrics[\"total_pixels\"]) * 100 if metrics[\"total_pixels\"] > 0 else 0\n",
    "        snr_iou = metrics[\"iou_sum\"] / metrics[\"iou_count\"] if metrics[\"iou_count\"] > 0 else 0\n",
    "        print(f\"SNR: {snr} dB - Accuracy: {snr_accuracy:.2f}%\")\n",
    "        print(f\"   IoU: {snr_iou:.4f}\")\n",
    "        for th in iou_thresholds:\n",
    "            recall = metrics[\"recall_counts\"][th] / metrics[\"total_samples\"][th] if metrics[\"total_samples\"][th] > 0 else 0\n",
    "            print(f\"   Recall at threshold {th}: {recall:.4f}\")\n",
    "\n",
    "    return snr_metrics\n",
    "\n",
    "\n",
    "def model_predictor(signals):\n",
    "    # Use the already loaded model and apply thresholding\n",
    "    return expand_true(model(signals) > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3aed7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run evaluation on the full dataset\n",
    "snr_metrics = evaluate(model_predictor, full_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3ba0e",
   "metadata": {},
   "source": [
    "### Save and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef69113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_results_and_plot(snr_metrics, save_path):\n",
    "    \"\"\"\n",
    "    Saves evaluation results to a JSON file and generates plots for Accuracy, IoU, and Recall vs. SNR.\n",
    "    Sets x-axis limits to range from -9 dB to 12 dB to eliminate blank space on the right.\n",
    "\n",
    "    Args:\n",
    "        snr_metrics (dict): The evaluation results obtained from the evaluate function.\n",
    "        save_path (str): The directory path where results and plots will be saved.\n",
    "\n",
    "    Outputs:\n",
    "        - evaluation_results.json\n",
    "        - accuracy_vs_snr.png and .svg\n",
    "        - iou_vs_snr.png and .svg\n",
    "        - recall_vs_snr.png and .svg\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Extract data from snr_metrics\n",
    "    snr_list = sorted(snr_metrics.keys())\n",
    "    accuracy_list = []\n",
    "    iou_list = []\n",
    "    recall_05 = []\n",
    "    recall_07 = []\n",
    "    recall_09 = []\n",
    "    \n",
    "    # Prepare data for JSON serialization\n",
    "    json_data = {}\n",
    "    \n",
    "    for snr in snr_list:\n",
    "        metrics = snr_metrics[snr]\n",
    "        snr_accuracy = (metrics[\"correct_pixels\"] / metrics[\"total_pixels\"]) * 100 if metrics[\"total_pixels\"] > 0 else 0\n",
    "        snr_iou = metrics[\"iou_sum\"] / metrics[\"iou_count\"] if metrics[\"iou_count\"] > 0 else 0\n",
    "        recall_at_05 = metrics[\"recall_counts\"][0.5] / metrics[\"total_samples\"][0.5] if metrics[\"total_samples\"][0.5] > 0 else 0\n",
    "        recall_at_07 = metrics[\"recall_counts\"][0.7] / metrics[\"total_samples\"][0.7] if metrics[\"total_samples\"][0.7] > 0 else 0\n",
    "        recall_at_09 = metrics[\"recall_counts\"][0.9] / metrics[\"total_samples\"][0.9] if metrics[\"total_samples\"][0.9] > 0 else 0\n",
    "\n",
    "        # Append to lists for plotting\n",
    "        accuracy_list.append(snr_accuracy)\n",
    "        iou_list.append(snr_iou)\n",
    "        recall_05.append(recall_at_05)\n",
    "        recall_07.append(recall_at_07)\n",
    "        recall_09.append(recall_at_09)\n",
    "\n",
    "        # Prepare data for JSON\n",
    "        json_data[snr] = {\n",
    "            \"accuracy\": snr_accuracy,\n",
    "            \"iou\": snr_iou,\n",
    "            \"recall\": {\n",
    "                \"0.5\": recall_at_05,\n",
    "                \"0.7\": recall_at_07,\n",
    "                \"0.9\": recall_at_09,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Save json_data to JSON file\n",
    "    json_file_path = os.path.join(save_path, 'evaluation_results.json')\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "    # Plot Accuracy vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, accuracy_list, marker='o', label='Accuracy')\n",
    "    plt.title('Accuracy vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    plt.xlim(-9, 12)\n",
    "    \n",
    "    # Save the plot\n",
    "    accuracy_png_path = os.path.join(save_path, 'accuracy_vs_snr.png')\n",
    "    accuracy_svg_path = os.path.join(save_path, 'accuracy_vs_snr.svg')\n",
    "    plt.savefig(accuracy_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(accuracy_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot IoU vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, iou_list, marker='o', color='orange', label='IoU')\n",
    "    plt.title('IoU vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    plt.xlim(-9, 12)\n",
    "    \n",
    "    # Save the plot\n",
    "    iou_png_path = os.path.join(save_path, 'iou_vs_snr.png')\n",
    "    iou_svg_path = os.path.join(save_path, 'iou_vs_snr.svg')\n",
    "    plt.savefig(iou_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(iou_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Recall at Different IoU Thresholds vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, recall_05, marker='o', label='Recall @ IoU 0.5')\n",
    "    plt.plot(snr_list, recall_07, marker='s', label='Recall @ IoU 0.7')\n",
    "    plt.plot(snr_list, recall_09, marker='^', label='Recall @ IoU 0.9')\n",
    "    plt.title('Recall at Different IoU Thresholds vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    plt.xlim(-9, 12)\n",
    "    \n",
    "    # Save the plot\n",
    "    recall_png_path = os.path.join(save_path, 'recall_vs_snr.png')\n",
    "    recall_svg_path = os.path.join(save_path, 'recall_vs_snr.svg')\n",
    "    plt.savefig(recall_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(recall_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9595d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming snr_metrics is the output from the evaluate function\n",
    "# Set the save path\n",
    "save_path = 'CMuSeNet_BIGRED_results'\n",
    "\n",
    "# Call the function\n",
    "save_results_and_plot(snr_metrics, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
