{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5007b71",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization block\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "STFT_LENGTH = 16 * 1024\n",
    "DATA_DIR = Path(\"/data/OTA_reduced/\")\n",
    "SAMPLE_RATE = 20e6\n",
    "MODULATIONS = [\"QPSK\", \"BPSK\", \"2-FSK\"]\n",
    "MODULATION_LABELS = {j: i for i, j in enumerate(MODULATIONS)}\n",
    "NUMBER_OF_MODULATIONS = len(MODULATIONS)\n",
    "MASK_SIZE = int(STFT_LENGTH)\n",
    "\n",
    "from matplotlib.mlab import psd as apply_psd\n",
    "\n",
    "def calc_sig_power(signal, meta, noise_power=-132.065):\n",
    "    \n",
    "    noise_floor_linear = 10 ** (noise_power / 10)\n",
    "    (psd, frequencies) = apply_psd(signal, Fs=SAMPLE_RATE, NFFT=1024)\n",
    "\n",
    "\n",
    "    signal_position = []\n",
    "\n",
    "    body = meta[\"body\"]\n",
    "    device = meta[\"client_id\"]\n",
    "    bandwidth, frequency_offset = body[\"bandwidth\"] + 20e3, body[\"frequency_offset\"]\n",
    "\n",
    "    \n",
    "    below_freq = frequency_offset-bandwidth/2\n",
    "    upper_freq = frequency_offset+bandwidth/2\n",
    "    sum_power_dbs = 0\n",
    "    freq_count = 0\n",
    "    \n",
    "    for idx, (power, freq) in enumerate(zip(psd, frequencies)):\n",
    "        if below_freq <= freq <= upper_freq:\n",
    "            freq_count+=1\n",
    "            sum_power_dbs+=(power)\n",
    "    return sum_power_dbs\n",
    "\n",
    "# noise_power is measured from noise signal collection\n",
    "def calc_snr(signal_power, noise_power=-132.065):\n",
    "    noise_floor_linear = 10 ** (noise_power / 10)\n",
    "    snr_linear = signal_power / (noise_floor_linear * 1024)\n",
    "    \n",
    "    snr_db = 10 * np.log10(snr_linear)\n",
    "    \n",
    "    return round(snr_db)\n",
    "\n",
    "def convert_metadata_format_real_to_simulated(signal, metadata):\n",
    "    name_mapping = {\"2FSK\": \"2-FSK\"}\n",
    "    return [\n",
    "        {\n",
    "            \"fc\": body[\"frequency_offset\"], \n",
    "            \"bw\": body[\"bandwidth\"] + 20e3,\n",
    "            \"mod\": name_mapping.get(body[\"modulation\"], body[\"modulation\"]),\n",
    "            \"snr\": calc_snr(calc_sig_power(signal, meta))\n",
    "        } for meta in metadata if (body := meta[\"body\"])\n",
    "    ]\n",
    "\n",
    "def load_data(signal_id, load_metadata_only=False):\n",
    "    if not load_metadata_only:\n",
    "        signal_path = DATA_DIR / str(signal_id) / \"data.npy\"\n",
    "        if not signal_path.exists():\n",
    "            raise FileNotFoundError(f\"Signal file {signal_path} not found.\")\n",
    "        signal = np.load(signal_path)\n",
    "    else:\n",
    "        signal = None\n",
    "    with open(DATA_DIR / str(signal_id) / \"meta-data.json\") as f:\n",
    "        meta = json.load(f)\n",
    "        if isinstance(meta, dict):\n",
    "            meta = [meta]\n",
    "    return signal, convert_metadata_format_real_to_simulated(signal, meta)\n",
    "\n",
    "\n",
    "    \n",
    "def _get_all_numbered_dirs(root_dir):\n",
    "    dirs = []\n",
    "    for directory in root_dir.iterdir():\n",
    "        dirs.append(int(directory.name))\n",
    "    dirs.sort()\n",
    "    return dirs\n",
    "        \n",
    "        \n",
    "def process_metadata(metadata):\n",
    "    scaled_metadata =  [\n",
    "        {\n",
    "            \"position\": (SAMPLE_RATE/2 + i['fc'], i['bw']),\n",
    "            \"mod\": i[\"mod\"],\n",
    "            \"snr\": i[\"snr\"],\n",
    "            \"bw\": int(i['bw'])\n",
    "        }\n",
    "        for i in metadata\n",
    "    ]\n",
    "    return scaled_metadata\n",
    "\n",
    "\n",
    "def process_signal(signal):\n",
    "    signal = signal[:STFT_LENGTH]\n",
    "\n",
    "    signal = np.fft.fft(signal)\n",
    "    signal = np.fft.fftshift(signal)\n",
    "    signal /= np.max(np.abs(signal))\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b802c",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WidebandSignalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, signal_ids, mask_size=MASK_SIZE, return_snrs=False):\n",
    "        self.mask_size = mask_size\n",
    "        self.signal_ids = signal_ids\n",
    "        self.return_snrs = return_snrs\n",
    "        self.snrs = []\n",
    "        loaded_data = []\n",
    "        \n",
    "        for signal_id in tqdm(self.signal_ids):\n",
    "            loaded_data.append(self.process_signal(signal_id))\n",
    "            \n",
    "        self.loaded_data = loaded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signal_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.return_snrs:\n",
    "            signal, masks, snr = self.loaded_data[index]\n",
    "        else:\n",
    "            signal, masks = self.loaded_data[index]\n",
    "\n",
    "        # Ensure `signal` is complex and `masks` is real-valued\n",
    "        if not isinstance(signal, torch.Tensor):\n",
    "            signal = torch.from_numpy(signal).type(torch.complex64)\n",
    "        if not isinstance(masks, torch.Tensor):\n",
    "            masks = torch.from_numpy(masks).type(torch.FloatTensor)\n",
    "\n",
    "        if self.return_snrs:\n",
    "            if not isinstance(snr, torch.Tensor):\n",
    "                snr = torch.tensor(snr).type(torch.FloatTensor)\n",
    "            return signal, masks, snr\n",
    "        else:\n",
    "            return signal, masks\n",
    "\n",
    "    def process_signal(self, signal_id):\n",
    "        # Load data and metadata\n",
    "        signal, metadata = load_data(signal_id)\n",
    "        \n",
    "        # Process the metadata and create masks\n",
    "        scaled_metadata = process_metadata(metadata)\n",
    "        snrs = [meta['snr'] for meta in scaled_metadata]\n",
    "        average_snr = sum(snrs) / len(snrs) if snrs else 0\n",
    "        \n",
    "        # Convert signal to complex format and normalize it\n",
    "        signal = process_signal(signal)  # `process_signal` should return np.ndarray (complex)\n",
    "        signal = torch.from_numpy(signal).type(torch.complex64)  # Convert to complex tensor\n",
    "        \n",
    "        # Generate binary mask for each frequency segment\n",
    "        masks = np.zeros(self.mask_size, dtype=np.float32)\n",
    "        scale_ratio = self.mask_size / SAMPLE_RATE\n",
    "        for meta in scaled_metadata:\n",
    "            f, b = meta['position']\n",
    "            x1 = math.floor((f - b / 2) * scale_ratio)\n",
    "            x2 = math.ceil((f + b / 2) * scale_ratio)\n",
    "            masks[x1:x2] = 1\n",
    "        \n",
    "        if self.return_snrs:\n",
    "            return signal, masks, average_snr\n",
    "        else:\n",
    "            return signal, masks\n",
    "\n",
    "\n",
    "# Train test split 80 - 10 - 10\n",
    "train, test, validation = [], [], [] \n",
    "total_signals = len([i for i in DATA_DIR.iterdir()])\n",
    "for index, signal in enumerate(_get_all_numbered_dirs(DATA_DIR)):\n",
    "    if index <= 0.80 * total_signals:\n",
    "        train.append(signal)\n",
    "    elif index <= 0.9 * total_signals:\n",
    "        validation.append(signal)\n",
    "    else:\n",
    "        test.append(signal)\n",
    "            \n",
    "print(\"Train\", len(train))\n",
    "print(\"Validation\", len(validation))\n",
    "print(\"Test\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74df1a",
   "metadata": {},
   "source": [
    "### Check if complex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f75344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_signal_loading(signal_id):\n",
    "    # Load a single signal and process it\n",
    "    signal, metadata = load_data(signal_id)\n",
    "    \n",
    "    # Process the signal: Apply any necessary preprocessing, and convert to complex format\n",
    "    processed_signal = process_signal(signal)  # This should return a complex np.ndarray\n",
    "    complex_signal = torch.from_numpy(processed_signal).type(torch.complex64)\n",
    "    \n",
    "    # Check if the signal is complex\n",
    "    print(\"Loaded Signal ID:\", signal_id)\n",
    "    print(\"Signal Type:\", complex_signal.dtype)\n",
    "    print(\"Signal Shape:\", complex_signal.shape)\n",
    "    \n",
    "    # Generate the mask as you would in WidebandSignalDataset\n",
    "    scaled_metadata = process_metadata(metadata)\n",
    "    masks = np.zeros(MASK_SIZE, dtype=np.float32)\n",
    "    scale_ratio = MASK_SIZE / SAMPLE_RATE\n",
    "    for meta in scaled_metadata:\n",
    "        f, b = meta['position']\n",
    "        x1 = math.floor((f - b / 2) * scale_ratio)\n",
    "        x2 = math.ceil((f + b / 2) * scale_ratio)\n",
    "        masks[x1:x2] = 1\n",
    "\n",
    "    # Convert mask to tensor\n",
    "    mask_tensor = torch.from_numpy(masks).type(torch.FloatTensor)\n",
    "\n",
    "    # Output information about the mask\n",
    "    print(\"Mask Shape:\", mask_tensor.shape)\n",
    "    print(\"Mask Type:\", mask_tensor.dtype)\n",
    "    \n",
    "    return complex_signal, mask_tensor\n",
    "\n",
    "# Test with a specific signal_id (replace with an actual ID from your data)\n",
    "test_signal_id = train[0]  # Assuming `train` list contains valid signal IDs\n",
    "complex_signal, mask_tensor = test_single_signal_loading(test_signal_id)\n",
    "\n",
    "# Optional: Check a sample value to confirm it's complex\n",
    "print(\"Sample value from signal tensor:\", complex_signal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WidebandSignalDataset(signal_ids=train)\n",
    "validation_dataset = WidebandSignalDataset(signal_ids=validation)\n",
    "test_dataset = WidebandSignalDataset(signal_ids=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0900d4e",
   "metadata": {},
   "source": [
    "### Check SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbee106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Train Dataset\n",
    "train_snrs = train_dataset.snrs\n",
    "\n",
    "# Plot Histogram of SNRs in Train Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_snrs, bins=range(int(min(train_snrs)), int(max(train_snrs)) + 1), edgecolor='black')\n",
    "plt.title('Histogram of SNRs in Train Dataset')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print SNR Range\n",
    "print('Train Dataset SNR range: {} dB to {} dB'.format(min(train_snrs), max(train_snrs)))\n",
    "\n",
    "# For Validation Dataset\n",
    "validation_snrs = validation_dataset.snrs\n",
    "\n",
    "# Plot Histogram of SNRs in Validation Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(validation_snrs, bins=range(int(min(validation_snrs)), int(max(validation_snrs)) + 1), edgecolor='black')\n",
    "plt.title('Histogram of SNRs in Validation Dataset')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print SNR Range\n",
    "print('Validation Dataset SNR range: {} dB to {} dB'.format(min(validation_snrs), max(validation_snrs)))\n",
    "\n",
    "# For Test Dataset\n",
    "test_snrs = test_dataset.snrs\n",
    "\n",
    "# Plot Histogram of SNRs in Validation Dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_snrs, bins=range(int(min(test_snrs)), int(max(test_snrs)) + 1), edgecolor='black')\n",
    "plt.title('Histogram of SNRs in Test Dataset')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print SNR Range\n",
    "print('Validation Dataset SNR range: {} dB to {} dB'.format(min(test_snrs), max(test_snrs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ae774",
   "metadata": {},
   "source": [
    "### Batch Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Updated batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train labels shape:\", len(train_dataset))\n",
    "print(\"Validation labels shape:\", len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e09e4",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f79a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0.0001, save_path='./path/to/model/save'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.best_model = None\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_model = model.state_dict()\n",
    "        save_path = os.path.join(self.save_path, 'best_model.pth')\n",
    "        torch.save(self.best_model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fda74",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import complexPyTorch.complexLayers as cplx\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def reshape_to_2d(data):\n",
    "    return data.view(-1, 1, 128, 128)  # Reshape to [batch, channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7562c",
   "metadata": {},
   "source": [
    "### Complex IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    real_pred = (pred.real > threshold).float()\n",
    "    imag_pred = (pred.imag > threshold).float()\n",
    "    \n",
    "    combined_pred = torch.logical_or(real_pred, imag_pred).float()\n",
    "    \n",
    "    intersection = (combined_pred * target).sum(dim=1)\n",
    "    union = (combined_pred + target).sum(dim=1) - intersection\n",
    "    iou = (intersection / union).mean().item()\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4063c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66825110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    iou_scores = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(valid_loader, desc=\"Validating\"):\n",
    "            inputs = reshape_to_2d(inputs).to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate IoU\n",
    "            iou = calculate_iou(outputs, masks, threshold=0.5)\n",
    "            iou_scores.append(iou)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (outputs.real > 0.5).float()\n",
    "            correct = (preds == masks).float().sum()\n",
    "            total_correct += correct.item()\n",
    "            total_samples += masks.numel()\n",
    "\n",
    "    val_loss = running_loss / len(valid_loader)\n",
    "    mean_iou = sum(iou_scores) / len(iou_scores)\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    print(f'Validation Loss: {val_loss:.6f}')\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return val_loss, accuracy\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, initial_lr=0.001, lr_steps=[0.000001], num_epochs=50, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    epoch_durations = []\n",
    "    \n",
    "    current_lr = initial_lr\n",
    "    for lr in lr_steps:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, delta=0.001)\n",
    "        print(\"Current learning rate: \", lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "                inputs = reshape_to_2d(inputs).to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            print(f\"Training Loss: {epoch_loss:.6f}\")\n",
    "            \n",
    "            val_loss, val_accuracy = validate_model(model, valid_loader, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            epoch_durations.append(epoch_duration)\n",
    "        if early_stopping.best_model is not None:\n",
    "            print(f\"Loading best model from lr {lr}\")\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "        \n",
    "    print(\"Training completed.\")\n",
    "    print(\"Epoch durations:\", epoch_durations)\n",
    "    return model, train_losses, val_losses, val_accuracies, epoch_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80cb51",
   "metadata": {},
   "source": [
    "### ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d208cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import complexPyTorch.complexLayers as cplx\n",
    "from typing import Optional, Callable, Type, Union, List\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> cplx.ComplexConv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return cplx.ComplexConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> cplx.ComplexConv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return cplx.ComplexConv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = cplx.ComplexBatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = cplx.ComplexBatchNorm2d(planes * self.expansion)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ComplexResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = STFT_LENGTH,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(ComplexResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = cplx.ComplexBatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = cplx.ComplexConv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = cplx.ComplexReLU()\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = cplx.ComplexLinear(512 * block.expansion, num_classes)\n",
    "        self.sigmoid = cplx.ComplexSigmoid()\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "def ComplexResNet18():\n",
    "    return ComplexResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Create the model instance\n",
    "model = ComplexResNet18()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc1b5d",
   "metadata": {},
   "source": [
    "### Complex focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c29429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(ComplexFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        real_inputs = inputs.real\n",
    "        imag_inputs = inputs.imag\n",
    "        \n",
    "        real_BCE_loss = F.binary_cross_entropy(real_inputs, targets, reduction='none')\n",
    "        imag_BCE_loss = F.binary_cross_entropy(imag_inputs, targets, reduction='none')\n",
    "        \n",
    "        real_pt = torch.exp(-real_BCE_loss)\n",
    "        imag_pt = torch.exp(-imag_BCE_loss)\n",
    "        \n",
    "        real_F_loss = self.alpha * (1 - real_pt) ** self.gamma * real_BCE_loss\n",
    "        imag_F_loss = self.alpha * (1 - imag_pt) ** self.gamma * imag_BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return (torch.mean(real_F_loss) + torch.mean(imag_F_loss)) / 2\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(real_F_loss) + torch.sum(imag_F_loss)\n",
    "        else:\n",
    "            return real_F_loss + imag_F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb35ba2",
   "metadata": {},
   "source": [
    "### Training with complex focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7526b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and train the ResNet-18 model\n",
    "model = ComplexResNet18().to(device)\n",
    "criterion = ComplexFocalLoss()\n",
    "\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations =train_model(model, train_loader, valid_loader, criterion, initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=3)\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c9d58",
   "metadata": {},
   "source": [
    "### CVNN RV-BCE and CV-BCE Loss function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c736b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV BCE Loss Function Definition\n",
    "class ComplexValuedBCELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(ComplexValuedBCELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        real_inputs = inputs.real\n",
    "        imag_inputs = inputs.imag\n",
    "\n",
    "        # Calculate binary cross-entropy for both real and imaginary parts\n",
    "        real_BCE_loss = F.binary_cross_entropy(real_inputs, targets, reduction=self.reduction)\n",
    "        imag_BCE_loss = F.binary_cross_entropy(imag_inputs, targets, reduction=self.reduction)\n",
    "        \n",
    "        # Combine the losses (you can adjust the weighting if necessary)\n",
    "        combined_BCE_loss = (real_BCE_loss + imag_BCE_loss) / 2\n",
    "        return combined_BCE_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d19ea7",
   "metadata": {},
   "source": [
    "### CV-BCE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56d5b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the criterion for CV BCE\n",
    "criterion = ComplexValuedBCELoss()\n",
    "\n",
    "# Train the ResNet-18 model with CV BCE\n",
    "device = torch.device('cuda')\n",
    "model = ComplexResNet18().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Start training with the previously defined train_model function\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations = train_model(\n",
    "    model, train_loader, valid_loader, criterion, \n",
    "    initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=3\n",
    ")\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd50ff",
   "metadata": {},
   "source": [
    "### Save and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define save directory\n",
    "save_dir = 'CMuSeNet_results/segmentation_OTA'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the training loss figure as PNG and SVG\n",
    "plt.savefig(os.path.join(save_dir, 'training_loss.png'))\n",
    "plt.savefig(os.path.join(save_dir, 'training_loss.svg'))\n",
    "\n",
    "# Show the training loss plot\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Save the validation accuracy figure as PNG and SVG\n",
    "plt.savefig(os.path.join(save_dir, 'validation_accuracy.png'))\n",
    "plt.savefig(os.path.join(save_dir, 'validation_accuracy.svg'))\n",
    "\n",
    "# Show the validation accuracy plot\n",
    "plt.show()\n",
    "\n",
    "# Save the actual data to a JSON file\n",
    "results = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_accuracies\": val_accuracies,\n",
    "    \"epoch_durations\": epoch_durations,\n",
    "    \"combined_epoch_time\": combined_epoch_time\n",
    "}\n",
    "\n",
    "# Save JSON file\n",
    "with open(os.path.join(save_dir, 'training_validation_results.json'), 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a757949",
   "metadata": {},
   "source": [
    "### Transfer Learning from Synthetic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee265d28",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block to load pre-trained model and prepare for transfer learning\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "\n",
    "model_path = \"path/to/model/save.pth\"\n",
    "model = ComplexResNet18().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for transfer learning (adjust `num_classes` as needed)\n",
    "num_classes = STFT_LENGTH  # Set based on your current task\n",
    "model.fc = cplx.ComplexLinear(512 * BasicBlock.expansion, num_classes).to(device)\n",
    "\n",
    "# Unfreeze the final layer for training\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1e62b",
   "metadata": {},
   "source": [
    "### Complex Learning for Transfer Learning (Same as above but easier access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2, reduction='mean'):\n",
    "        super(ComplexFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        real_inputs = inputs.real\n",
    "        imag_inputs = inputs.imag\n",
    "        \n",
    "        real_BCE_loss = F.binary_cross_entropy(real_inputs, targets, reduction='none')\n",
    "        imag_BCE_loss = F.binary_cross_entropy(imag_inputs, targets, reduction='none')\n",
    "        \n",
    "        real_pt = torch.exp(-real_BCE_loss)\n",
    "        imag_pt = torch.exp(-imag_BCE_loss)\n",
    "        \n",
    "        real_F_loss = self.alpha * (1 - real_pt) ** self.gamma * real_BCE_loss\n",
    "        imag_F_loss = self.alpha * (1 - imag_pt) ** self.gamma * imag_BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return (torch.mean(real_F_loss) + torch.mean(imag_F_loss)) / 2\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(real_F_loss) + torch.sum(imag_F_loss)\n",
    "        else:\n",
    "            return real_F_loss + imag_F_loss\n",
    "\n",
    "# Update the IoU calculation to handle complex values\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    real_pred = (pred.real > threshold).float()\n",
    "    imag_pred = (pred.imag > threshold).float()\n",
    "    \n",
    "    combined_pred = torch.logical_or(real_pred, imag_pred).float()\n",
    "    \n",
    "    intersection = (combined_pred * target).sum(dim=1)\n",
    "    union = (combined_pred + target).sum(dim=1) - intersection\n",
    "    iou = (intersection / union).mean().item()\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b7701",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291a42e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a new criterion and optimizer for fine-tuning\n",
    "# You may select between Focal Loss or BCE as your criterion\n",
    "#criterion = ComplexValuedBCELoss()  # or ComplexValuedBCELoss()\n",
    "criterion = ComplexFocalLoss()\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Train the model (fine-tuning)\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations= train_model(\n",
    "    model, train_loader, valid_loader, criterion,\n",
    "    initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=3\n",
    ")\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f81acc",
   "metadata": {},
   "source": [
    "## Transfer Transfer Learning (Different Radio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55017794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block to load pre-trained model and prepare for transfer learning\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_path = \"/path/to/model/save.pth\"\n",
    "model = ComplexResNet18().to(device)\n",
    "#model = ComplexValuedBCELoss().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for transfer learning (adjust `num_classes` as needed)\n",
    "num_classes = STFT_LENGTH  # Set based on your current task\n",
    "model.fc = cplx.ComplexLinear(512 * BasicBlock.expansion, num_classes).to(device)\n",
    "\n",
    "# Unfreeze the final layer for training\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new criterion and optimizer for fine-tuning\n",
    "# You may select between Focal Loss or BCE as your criterion\n",
    "#criterion = ComplexValuedBCELoss()  # or ComplexValuedBCELoss()\n",
    "criterion = ComplexFocalLoss()\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Train the model (fine-tuning)\n",
    "model, train_losses, val_losses, val_accuracies, epoch_durations= train_model(\n",
    "    model, train_loader, valid_loader, criterion,\n",
    "    initial_lr=0.001, lr_steps=[0.001, 0.0001], num_epochs=50, patience=1\n",
    ")\n",
    "combined_epoch_time = sum(epoch_durations)\n",
    "print(f\"Total time spent in epochs: {combined_epoch_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef8bad",
   "metadata": {},
   "source": [
    "### Evaluation CVNN OTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model_path = \"/path/to/model/save.pth\"\n",
    "model = ComplexResNet18().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for evaluation\n",
    "\n",
    "full_dataset = ConcatDataset([\n",
    "    WidebandSignalDataset(signal_ids=train, return_snrs=True),\n",
    "    WidebandSignalDataset(signal_ids=validation, return_snrs=True),\n",
    "    WidebandSignalDataset(signal_ids=test, return_snrs=True)\n",
    "])\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad326f1d",
   "metadata": {},
   "source": [
    "### Function initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_true(array, distance=1):\n",
    "    # Create kernel of appropriate size\n",
    "    kernel = torch.ones((1, 1, distance * 2 + 1), device=array.device)\n",
    "    array = array.unsqueeze(1).float()  # Add channel dimension\n",
    "    result = F.conv1d(array, kernel, padding=distance)\n",
    "    result = result.squeeze(1)  # Remove the extra dimension\n",
    "    return result > 0\n",
    "def reshape_to_2d(data):\n",
    "    return data.view(-1, 1, 128, 128)  # Reshape to [batch, channels, height, width]\n",
    "def get_true_groups(tensor, device):\n",
    "    assert tensor.dim() == 2, 'This function handles 2D tensor only'\n",
    "    all_groups = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        item = tensor[i]\n",
    "        item = torch.cat([torch.tensor([False]).to(device), item, torch.tensor([False]).to(device)])\n",
    "        diffs = item.float().diff()\n",
    "        starts = (diffs == 1).nonzero(as_tuple=True)[0]\n",
    "        ends = (diffs == -1).nonzero(as_tuple=True)[0] - 1\n",
    "        groups = [(start.item(), end.item()) for start, end in zip(starts, ends)]\n",
    "        all_groups.append(groups)\n",
    "    return all_groups\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    intersection = max(0, min(box1[1], box2[1]) - max(box1[0], box2[0]))\n",
    "    union = max(box1[1], box2[1]) - min(box1[0], box2[0])\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def match_targets(targets, preds):\n",
    "    ious = []\n",
    "    for target in targets:\n",
    "        iou_targets = []\n",
    "        for pred in preds:\n",
    "            iou_targets.append(calculate_iou(target, pred))\n",
    "        ious.append(iou_targets)\n",
    "    cost_matrix = np.array(ious)\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "    return row_ind, col_ind\n",
    "\n",
    "def calculate_matched_ious(target_boxes, prediction_boxes, matching):\n",
    "    ious = [0 for _ in target_boxes]\n",
    "    matching_dict = dict(zip(*matching))\n",
    "    for target_index, target_box in enumerate(target_boxes):\n",
    "        if target_index in matching_dict:\n",
    "            pred_index = matching_dict[target_index]\n",
    "            if pred_index < len(prediction_boxes):\n",
    "                box1 = target_box\n",
    "                box2 = prediction_boxes[pred_index]\n",
    "                ious[target_index] = calculate_iou(box1, box2)\n",
    "    return ious\n",
    "def model_predictor(signals):\n",
    "    # Convert signals to complex tensors\n",
    "    if signals.dtype != torch.complex64 and signals.dtype != torch.complex128:\n",
    "        signals = signals.type(torch.complex64)\n",
    "    # Reshape the input signals to the expected shape\n",
    "    signals = reshape_to_2d(signals)\n",
    "    signals = signals.to(device)\n",
    "    # Use the already loaded model and apply thresholding\n",
    "    with torch.no_grad():\n",
    "        outputs = model(signals)\n",
    "    # Handle complex outputs appropriately\n",
    "    real_outputs = outputs.real\n",
    "    imag_outputs = outputs.imag\n",
    "    real_pred = (real_outputs > 0.5)\n",
    "    imag_pred = (imag_outputs > 0.5)\n",
    "    combined_pred = torch.logical_or(real_pred, imag_pred)\n",
    "    return expand_true(combined_pred.float())\n",
    "\n",
    "# Complex IoU Implementation\n",
    "def calculate_complex_iou(box1_real, box1_imag, box2_real, box2_imag):\n",
    "    # Calculate real component intersection\n",
    "    real_intersection = max(0, min(box1_real[1], box2_real[1]) - max(box1_real[0], box2_real[0]))\n",
    "    real_union = max(box1_real[1], box2_real[1]) - min(box1_real[0], box2_real[0])\n",
    "    \n",
    "    # Calculate imaginary component intersection\n",
    "    imag_intersection = max(0, min(box1_imag[1], box2_imag[1]) - max(box1_imag[0], box2_imag[0]))\n",
    "    imag_union = max(box1_imag[1], box2_imag[1]) - min(box1_imag[0], box2_imag[0])\n",
    "    \n",
    "    # Combine intersections and unions\n",
    "    total_intersection = real_intersection + imag_intersection\n",
    "    total_union = real_union + imag_union\n",
    "    \n",
    "    # Return IoU\n",
    "    return total_intersection / total_union if total_union != 0 else 0\n",
    "\n",
    "def match_complex_targets(targets_real, targets_imag, preds_real, preds_imag):\n",
    "    ious = []\n",
    "    for target_real, target_imag in zip(targets_real, targets_imag):\n",
    "        iou_targets = []\n",
    "        for pred_real, pred_imag in zip(preds_real, preds_imag):\n",
    "            iou_targets.append(calculate_complex_iou(target_real, target_imag, pred_real, pred_imag))\n",
    "        ious.append(iou_targets)\n",
    "    cost_matrix = np.array(ious)\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "    return row_ind, col_ind\n",
    "\n",
    "def calculate_matched_complex_ious(target_boxes_real, target_boxes_imag, \n",
    "                                   prediction_boxes_real, prediction_boxes_imag, matching):\n",
    "    ious = [0 for _ in target_boxes_real]\n",
    "    matching_dict = dict(zip(*matching))\n",
    "    for target_index, (target_box_real, target_box_imag) in enumerate(zip(target_boxes_real, target_boxes_imag)):\n",
    "        if target_index in matching_dict:\n",
    "            pred_index = matching_dict[target_index]\n",
    "            if pred_index < len(prediction_boxes_real):\n",
    "                box1_real, box1_imag = target_box_real, target_box_imag\n",
    "                box2_real, box2_imag = prediction_boxes_real[pred_index], prediction_boxes_imag[pred_index]\n",
    "                ious[target_index] = calculate_complex_iou(box1_real, box1_imag, box2_real, box2_imag)\n",
    "    return ious\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114c7a2",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f12e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictor, data_loader, device=\"cuda\"):\n",
    "    iou_thresholds = [0.5, 0.7, 0.9]\n",
    "    snr_metrics = defaultdict(lambda: {\n",
    "        \"iou_sum\": 0.0,\n",
    "        \"iou_count\": 0,\n",
    "        \"recall_counts\": defaultdict(int),\n",
    "        \"total_samples\": defaultdict(int),\n",
    "        \"correct_pixels\": 0,\n",
    "        \"total_pixels\": 0\n",
    "    })\n",
    "    total_iou_sum, total_iou_count = 0.0, 0\n",
    "    total_correct_pixels, total_total_pixels = 0, 0\n",
    "    total_recall_counts = defaultdict(int)\n",
    "    total_samples = defaultdict(int)\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        if len(batch) == 3:\n",
    "            inputs, masks, snrs_in_batch = batch\n",
    "        else:\n",
    "            inputs, masks = batch\n",
    "            snrs_in_batch = [0] * len(inputs)  # Default SNR if not provided\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = predictor(inputs)\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "            mask = masks[i]\n",
    "            output = outputs[i]\n",
    "\n",
    "            # Resize output to match mask shape if necessary\n",
    "            if output.numel() != mask.numel():\n",
    "                output = output.expand_as(mask) if output.numel() == 1 else output.reshape_as(mask)\n",
    "\n",
    "            thresholded_output = (output >= 0.5).float()\n",
    "\n",
    "            correct_pixels = (thresholded_output == mask).sum().item()\n",
    "            total_pixels = mask.numel()\n",
    "            total_correct_pixels += correct_pixels\n",
    "            total_total_pixels += total_pixels\n",
    "\n",
    "            # Get SNR value and round it to the nearest integer\n",
    "            snr = snrs_in_batch[i]\n",
    "            if isinstance(snr, torch.Tensor):\n",
    "                snr = snr.item()\n",
    "            snr = int(round(snr))  # Round SNR to the nearest integer\n",
    "\n",
    "            snr_metrics[snr][\"correct_pixels\"] += correct_pixels\n",
    "            snr_metrics[snr][\"total_pixels\"] += total_pixels\n",
    "\n",
    "            target_boxes = get_true_groups(mask.unsqueeze(0), device=device)[0]\n",
    "            pred_boxes = get_true_groups(thresholded_output.unsqueeze(0), device=device)[0]\n",
    "            if not target_boxes or not pred_boxes:\n",
    "                continue\n",
    "            matching = match_targets(target_boxes, pred_boxes)\n",
    "            matched_ious = calculate_matched_ious(target_boxes, pred_boxes, matching)\n",
    "\n",
    "            snr_metrics[snr][\"iou_sum\"] += sum(matched_ious)\n",
    "            snr_metrics[snr][\"iou_count\"] += len(matched_ious)\n",
    "            total_iou_sum += sum(matched_ious)\n",
    "            total_iou_count += len(matched_ious)\n",
    "\n",
    "            for th in iou_thresholds:\n",
    "                true_positives = sum(1 for iou in matched_ious if iou >= th)\n",
    "                snr_metrics[snr][\"recall_counts\"][th] += true_positives\n",
    "                snr_metrics[snr][\"total_samples\"][th] += len(target_boxes)\n",
    "                total_recall_counts[th] += true_positives\n",
    "                total_samples[th] += len(target_boxes)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = (total_correct_pixels / total_total_pixels) * 100 if total_total_pixels > 0 else 0\n",
    "    overall_iou = total_iou_sum / total_iou_count if total_iou_count > 0 else 0\n",
    "    overall_recall = {\n",
    "        th: total_recall_counts[th] / total_samples[th] if total_samples[th] > 0 else 0\n",
    "        for th in iou_thresholds\n",
    "    }\n",
    "\n",
    "    # Print overall results\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(f\"Overall IoU Score: {overall_iou:.4f}\")\n",
    "    for th in iou_thresholds:\n",
    "        print(f\"Recall at threshold {th}: {overall_recall[th]:.4f}\")\n",
    "\n",
    "    # Print per-SNR results\n",
    "    for snr in sorted(snr_metrics.keys()):\n",
    "        metrics = snr_metrics[snr]\n",
    "        snr_accuracy = (metrics[\"correct_pixels\"] / metrics[\"total_pixels\"]) * 100 if metrics[\"total_pixels\"] > 0 else 0\n",
    "        snr_iou = metrics[\"iou_sum\"] / metrics[\"iou_count\"] if metrics[\"iou_count\"] > 0 else 0\n",
    "        print(f\"SNR: {snr} dB - Accuracy: {snr_accuracy:.2f}%\")\n",
    "        print(f\"   IoU: {snr_iou:.4f}\")\n",
    "        for th in iou_thresholds:\n",
    "            recall = metrics[\"recall_counts\"][th] / metrics[\"total_samples\"][th] if metrics[\"total_samples\"][th] > 0 else 0\n",
    "            print(f\"   Recall at threshold {th}: {recall:.4f}\")\n",
    "\n",
    "    return snr_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fd13f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run evaluation on the full dataset\n",
    "snr_metrics = evaluate(model_predictor, full_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eade04",
   "metadata": {},
   "source": [
    "### Save and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_results_and_plot(snr_metrics, save_path):\n",
    "    \"\"\"\n",
    "    Saves evaluation results to a JSON file and generates plots for Accuracy, IoU, and Recall vs. SNR.\n",
    "    Sets x-axis limits to range from -9 dB to 12 dB to eliminate blank space on the right.\n",
    "\n",
    "    Args:\n",
    "        snr_metrics (dict): The evaluation results obtained from the evaluate function.\n",
    "        save_path (str): The directory path where results and plots will be saved.\n",
    "\n",
    "    Outputs:\n",
    "        - evaluation_results.json\n",
    "        - accuracy_vs_snr.png and .svg\n",
    "        - iou_vs_snr.png and .svg\n",
    "        - recall_vs_snr.png and .svg\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Extract data from snr_metrics\n",
    "    snr_list = sorted(snr_metrics.keys())\n",
    "    accuracy_list = []\n",
    "    iou_list = []\n",
    "    recall_05 = []\n",
    "    recall_07 = []\n",
    "    recall_09 = []\n",
    "    \n",
    "    # Prepare data for JSON serialization\n",
    "    json_data = {}\n",
    "    \n",
    "    for snr in snr_list:\n",
    "        metrics = snr_metrics[snr]\n",
    "        snr_accuracy = (metrics[\"correct_pixels\"] / metrics[\"total_pixels\"]) * 100 if metrics[\"total_pixels\"] > 0 else 0\n",
    "        snr_iou = metrics[\"iou_sum\"] / metrics[\"iou_count\"] if metrics[\"iou_count\"] > 0 else 0\n",
    "        recall_at_05 = metrics[\"recall_counts\"][0.5] / metrics[\"total_samples\"][0.5] if metrics[\"total_samples\"][0.5] > 0 else 0\n",
    "        recall_at_07 = metrics[\"recall_counts\"][0.7] / metrics[\"total_samples\"][0.7] if metrics[\"total_samples\"][0.7] > 0 else 0\n",
    "        recall_at_09 = metrics[\"recall_counts\"][0.9] / metrics[\"total_samples\"][0.9] if metrics[\"total_samples\"][0.9] > 0 else 0\n",
    "\n",
    "        # Append to lists for plotting\n",
    "        accuracy_list.append(snr_accuracy)\n",
    "        iou_list.append(snr_iou)\n",
    "        recall_05.append(recall_at_05)\n",
    "        recall_07.append(recall_at_07)\n",
    "        recall_09.append(recall_at_09)\n",
    "\n",
    "        # Prepare data for JSON\n",
    "        json_data[snr] = {\n",
    "            \"accuracy\": snr_accuracy,\n",
    "            \"iou\": snr_iou,\n",
    "            \"recall\": {\n",
    "                \"0.5\": recall_at_05,\n",
    "                \"0.7\": recall_at_07,\n",
    "                \"0.9\": recall_at_09,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Save json_data to JSON file\n",
    "    json_file_path = os.path.join(save_path, 'evaluation_results.json')\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "    # Plot Accuracy vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, accuracy_list, marker='o', label='Accuracy')\n",
    "    plt.title('Accuracy vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    #plt.xlim(-9, 12)\n",
    "    plt.xlim(-16, 16)\n",
    "    # Save the plot\n",
    "    accuracy_png_path = os.path.join(save_path, 'accuracy_vs_snr.png')\n",
    "    accuracy_svg_path = os.path.join(save_path, 'accuracy_vs_snr.svg')\n",
    "    plt.savefig(accuracy_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(accuracy_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot IoU vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, iou_list, marker='o', color='orange', label='IoU')\n",
    "    plt.title('IoU vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    #plt.xlim(-9, 12)\n",
    "    plt.xlim(-16, 16)\n",
    "    # Save the plot\n",
    "    iou_png_path = os.path.join(save_path, 'iou_vs_snr.png')\n",
    "    iou_svg_path = os.path.join(save_path, 'iou_vs_snr.svg')\n",
    "    plt.savefig(iou_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(iou_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Recall at Different IoU Thresholds vs. SNR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(snr_list, recall_05, marker='o', label='Recall @ IoU 0.5')\n",
    "    plt.plot(snr_list, recall_07, marker='s', label='Recall @ IoU 0.7')\n",
    "    plt.plot(snr_list, recall_09, marker='^', label='Recall @ IoU 0.9')\n",
    "    plt.title('Recall at Different IoU Thresholds vs. SNR')\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    plt.xlim(-9, 12)\n",
    "    \n",
    "    # Save the plot\n",
    "    recall_png_path = os.path.join(save_path, 'recall_vs_snr.png')\n",
    "    recall_svg_path = os.path.join(save_path, 'recall_vs_snr.svg')\n",
    "    plt.savefig(recall_png_path, format='png', bbox_inches='tight')\n",
    "    plt.savefig(recall_svg_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974e70d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = 'CMuSeNet_results/OTA'\n",
    "\n",
    "# Save results and generate plots\n",
    "save_results_and_plot(snr_metrics, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
